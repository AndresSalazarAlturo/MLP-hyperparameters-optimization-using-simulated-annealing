{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "\n",
    "# Adjustment of visibility of Datafreames\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "pd.set_option('display.width', 500)\n",
    "\n",
    "df = pd.read_csv(\"./data/diabetes.csv\")\n",
    "\n",
    "def grab_col_names(dataframe, cat_th=10, car_th=20):\n",
    "    # cat_cols, cat_but_car\n",
    "    cat_cols = [col for col in dataframe.columns if dataframe[col].dtypes == \"O\"]\n",
    "    num_but_cat = [col for col in dataframe.columns if dataframe[col].nunique() < cat_th and\n",
    "                   dataframe[col].dtypes != \"O\"]\n",
    "    cat_but_car = [col for col in dataframe.columns if dataframe[col].nunique() > car_th and\n",
    "                   dataframe[col].dtypes == \"O\"]\n",
    "    cat_cols = cat_cols + num_but_cat\n",
    "    cat_cols = [col for col in cat_cols if col not in cat_but_car]\n",
    "\n",
    "    # num_cols\n",
    "    num_cols = [col for col in dataframe.columns if dataframe[col].dtypes != \"O\"]\n",
    "    num_cols = [col for col in num_cols if col not in num_but_cat]\n",
    "\n",
    "    # report\n",
    "    print(f\"Observations: {dataframe.shape[0]}\")\n",
    "    print(f\"Variables: {dataframe.shape[1]}\")\n",
    "    print(f'cat_cols: {len(cat_cols)}')         # the number of categorical variables\n",
    "    print(f'num_cols: {len(num_cols)}')         # the number of numerical variables\n",
    "    print(f'cat_but_car: {len(cat_but_car)}')   # the number of cardinal variables\n",
    "    print(f'num_but_cat: {len(num_but_cat)}')   # the number of categorical variables that looks numerical\n",
    "    return cat_cols, num_cols, cat_but_car\n",
    "\n",
    "\n",
    "cat_cols, num_cols, cat_but_car = grab_col_names(df)\n",
    "\n",
    "num_cols_miss = [i for i in num_cols if i != \"Pregnancies\"]\n",
    "for i in num_cols_miss:\n",
    "    df[i] = df.apply(lambda x: np.nan if x[i] == 0 else x[i], axis=1)\n",
    "\n",
    "df_fill = df.apply(lambda x: x.fillna(x.median()) if x.dtype != \"O\" else x, axis=0)\n",
    "\n",
    "data_X = df_fill.loc[:, df_fill.columns != \"Outcome\"]\n",
    "data_Y = df_fill[[\"Outcome\"]]\n",
    "\n",
    "train_X, test_X, train_Y, test_Y = train_test_split(data_X, data_Y,\n",
    "                                                    test_size=0.2,\n",
    "                                                    stratify=data_Y,\n",
    "                                                    random_state=0)\n",
    "\n",
    "train_X.reset_index(drop=True, inplace=True)\n",
    "test_X.reset_index(drop=True, inplace=True)\n",
    "train_Y.reset_index(drop=True, inplace=True)\n",
    "test_Y.reset_index(drop=True, inplace=True)\n",
    "\n",
    "data_Y[\"Outcome\"].value_counts()\n",
    "\n",
    "feature_names = train_X.columns\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "## Fit to train_X\n",
    "scaler.fit(train_X)\n",
    "\n",
    "## transform train_X\n",
    "train_X = scaler.transform(train_X)\n",
    "train_X = pd.DataFrame(train_X, columns = feature_names)\n",
    "\n",
    "## transform test_X\n",
    "test_X = scaler.transform(test_X)\n",
    "test_X = pd.DataFrame(test_X, columns = feature_names)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimulatedAnnealing:\n",
    "\n",
    "    def __init__(self, initial_temp, final_temp, cooling_rate, max_iterations, kernel_mutation_prob, \n",
    "                 solver_set, activation_set, hidden_layer_sizes_set, alpha_set, learning_rate_init_set):\n",
    "        \"\"\"\n",
    "            Initialize simulated annealing variables.\n",
    "        \"\"\"\n",
    "\n",
    "        self.initial_temp = initial_temp\n",
    "        self.final_temp = final_temp\n",
    "        self.cooling_rate = cooling_rate\n",
    "        self.max_iterations = max_iterations\n",
    "        self.kernel_mutation_prob = kernel_mutation_prob\n",
    "\n",
    "        self.fitness_over_time = []\n",
    "        self.temps = []\n",
    "        self.genome_over_time = []\n",
    "        self.best_fitness_overtime = {}\n",
    "\n",
    "        ## Evolved Hyperparameters\n",
    "        self.solver = solver_set\n",
    "        self.activation = activation_set\n",
    "        self.hidden_layer_sizes = hidden_layer_sizes_set\n",
    "        self.alpha = alpha_set\n",
    "        self.learning_rate_init = learning_rate_init_set\n",
    "\n",
    "    def accept_solution(self, change_in_fitness, temperature):\n",
    "        \"\"\"\n",
    "            Accept the change in fitnees or not.\n",
    "            :param change_in_fitness: The change in fitness --> New fitness - current fitness\n",
    "            :param temperature: The temperature in that iteration\n",
    "            :return: True is the change is accepted else False\n",
    "        \"\"\"\n",
    "\n",
    "        ## If the change in fitness is greater than 0, the change is always accepted\n",
    "        if change_in_fitness > 0:\n",
    "            return True\n",
    "        else:\n",
    "        ## If the change in fitness is less than cero, accept the change with some probability\n",
    "            p = np.exp(change_in_fitness / temperature)\n",
    "            if np.random.rand() < p:\n",
    "                ## Accept the change - return True\n",
    "                return True\n",
    "            else:\n",
    "                ## Reject the change - return False\n",
    "                return False\n",
    "\n",
    "    def create_random_hyperparameters_set(self, my_solver, my_activation, my_hidden_layer_sizes, my_alpha, my_learing_rate_init):\n",
    "        \"\"\"\n",
    "            Create random hyperparameters set for MLP model. Uses the previously defined possible parameters to randomly create the first genotype.\n",
    "            :param solver: List with possible solvers\n",
    "            :param activation: List with possible activation functions\n",
    "            :param hidden_layer_sizes: Range of possible hidden layer sizes\n",
    "            :param alpha: Range of possible alpha values\n",
    "            :param learning_rate_init: Range of possible learning rate values\n",
    "\n",
    "            :return: List with random MLP hyperparameter values\n",
    "        \"\"\"\n",
    "\n",
    "        ## Create the output list\n",
    "        hyper_params = [random.choice(my_solver), random.choice(my_activation), random.choice(my_hidden_layer_sizes), random.choice(my_alpha), random.choice(my_learing_rate_init)]\n",
    "\n",
    "        return hyper_params\n",
    "\n",
    "    def get_mlp_fitness(self, my_solver, my_activation, my_hidden_layer_sizes, my_alpha, my_learing_rate_init):\n",
    "        \"\"\"\n",
    "            Cretes MLP model, train and test the model to get the model fitness\n",
    "            :param solver: Selected MLP solver\n",
    "            :param activation: Selected activation function\n",
    "            :param hidden_layer_sizes: Selected hidden layer sizes value\n",
    "            :param alpha: Selected alpha value\n",
    "            :param learning_rate_init: Selected learning rate\n",
    "\n",
    "            :return: MLP F1 Score\n",
    "        \"\"\"\n",
    "\n",
    "        my_mlp = MLPClassifier(solver=my_solver, activation=my_activation, hidden_layer_sizes=my_hidden_layer_sizes, alpha=my_alpha, learning_rate_init=my_learing_rate_init, max_iter=10000)\n",
    "        my_mlp.fit(train_X, train_Y.values.ravel())\n",
    "        my_prediction = my_mlp.predict(test_X)\n",
    "        my_f1_score = f1_score(test_Y, my_prediction, average='micro')\n",
    "\n",
    "        return my_f1_score\n",
    "\n",
    "    def random_number_close_range(self, current_value, x, int_value = False):\n",
    "        \"\"\"\n",
    "            Take the current value, add and subtract a constant value x to generate a randon\n",
    "            value in the range(current_value - x, current_value + x)\n",
    "            :param current_value: Current value that is the set to be the main value in the range\n",
    "            :param x: Value to add and subtract to current_value\n",
    "\n",
    "            :return: Random number in range(current_value - x, current_value + x)\n",
    "        \"\"\"\n",
    "\n",
    "        lower_bound = current_value - x\n",
    "        upper_bound = current_value + x\n",
    "\n",
    "        if lower_bound < 0:\n",
    "            lower_bound = 0\n",
    "\n",
    "        if int_value:\n",
    "            ## Generate random integer number between lower and upper bounds\n",
    "            random_rational_number = random.randint(lower_bound, upper_bound)\n",
    "\n",
    "        else:\n",
    "            ## Generate random rational number between lower and upper bounds\n",
    "            random_rational_number = random.uniform(lower_bound, upper_bound)\n",
    "\n",
    "        return random_rational_number\n",
    "\n",
    "    def optimization(self):\n",
    "        \"\"\"\n",
    "            Simulated annealing process.\n",
    "        \"\"\"\n",
    "\n",
    "        ## Create a current random solution\n",
    "        current_solution = self.create_random_hyperparameters_set(self.solver, self.activation, self.hidden_layer_sizes, self.alpha, self.learning_rate_init)\n",
    "\n",
    "        ## Current fitness\n",
    "        current_fitness = self.get_mlp_fitness(my_solver=current_solution[0], my_activation=current_solution[1], my_hidden_layer_sizes=current_solution[2], my_alpha=current_solution[3], my_learing_rate_init=current_solution[4])\n",
    "\n",
    "        n_genes = len(current_solution)\n",
    "        self.mutation_rate = 1/n_genes\n",
    "\n",
    "        ## Set the initial temperature\n",
    "        temperature = self.initial_temp\n",
    "\n",
    "        ## We create a fitness_over_time that store all the fitness calculated, to keep track in the changes\n",
    "        ## A copy is used, as if is not use the case where the change is not accepted, is going to change the \n",
    "        ## current_fitness, but using .copy() does not change the original value of current_fitness\n",
    "        self.fitness_over_time.append(current_fitness.copy())\n",
    "        ## Store the temperature through iterations\n",
    "        self.temps.append(temperature)\n",
    "        ## Store the genotype through iterations\n",
    "        self.genome_over_time.append(current_solution.copy())\n",
    "\n",
    "        ###########################################################\n",
    "\n",
    "        ## Record start time\n",
    "        start = time.time()\n",
    "\n",
    "        ## Do it until reach the maximum iterations\n",
    "        for iteration in range(self.max_iterations):\n",
    "\n",
    "            # Generate a neighboring solution with mutation (gene flipping)\n",
    "            new_solution = current_solution.copy()\n",
    "\n",
    "            ## Calculate the probability of mutation for each gene and modify it if the random\n",
    "            ## number generated is lower than the mutation_rate.\n",
    "            ## This one could flip more than one gene, althought is more likely to not change \n",
    "            ## any gen due to the low probability\n",
    "            for gene_no in range(len(new_solution)):\n",
    "                \n",
    "                if np.random.rand() < self.mutation_rate:\n",
    "\n",
    "                    if gene_no == 0 and np.random.rand() < self.kernel_mutation_prob:\n",
    "                        new_solution[gene_no] = random.choice(self.solver)\n",
    "                    elif gene_no == 1 and np.random.rand() < self.kernel_mutation_prob:\n",
    "                        new_solution[gene_no] = random.choice(self.activation)\n",
    "                    elif gene_no == 2:\n",
    "                        # new_solution[gene_no] = random.choice(self.hidden_layer_sizes)\n",
    "                        new_solution[gene_no] = self.random_number_close_range(new_solution[gene_no], 2, int_value=True)\n",
    "                    elif gene_no == 3:\n",
    "                        # new_solution[gene_no] = random.choice(self.alpha)\n",
    "                        new_solution[gene_no] = self.random_number_close_range(new_solution[gene_no], 0.01)\n",
    "                    elif gene_no == 4:\n",
    "                        # new_solution[gene_no] = random.choice(self.learning_rate_init)\n",
    "                        new_solution[gene_no] = self.random_number_close_range(new_solution[gene_no], 0.001)\n",
    "\n",
    "            ## Calculate the fitness for the new solution\n",
    "            new_fitness = self.get_mlp_fitness(my_solver=new_solution[0], my_activation=new_solution[1], my_hidden_layer_sizes=new_solution[2], my_alpha=new_solution[3], my_learing_rate_init=new_solution[4])\n",
    "            print(f\"new_fitness: {new_fitness}\")\n",
    "\n",
    "            ## Calculate the change in the fitness, use to know whether to accepted or not later\n",
    "            change_in_fitness = new_fitness - current_fitness\n",
    "            print(f\"change_in_fitness: {change_in_fitness}\")\n",
    "\n",
    "            ## Check if new solution should be accepted, if not accepted just go to the next iteration\n",
    "            ## and ignore the new fitness\n",
    "            if self.accept_solution(change_in_fitness, temperature):\n",
    "                current_solution = new_solution.copy()\n",
    "                current_fitness = new_fitness.copy()\n",
    "            \n",
    "            print(f\"Current solution: {current_solution}\")\n",
    "\n",
    "            ## Add the new fitness to fitness_over_time to plot the changes in fitness later\n",
    "            self.fitness_over_time.append(current_fitness.copy())\n",
    "            print(f\"fitness_over_time: {self.fitness_over_time}\")\n",
    "\n",
    "            ## Track solutions with high fitness\n",
    "            print(f\"Current fitness: {current_fitness}\")\n",
    "            if current_fitness >= 0.82:\n",
    "                self.best_fitness_overtime[current_fitness] = current_solution\n",
    "\n",
    "            ## Cool down\n",
    "            temperature *= self.cooling_rate\n",
    "            self.temps.append(temperature)\n",
    "            print(f\"temps: {self.temps}\")\n",
    "\n",
    "            print(f\"iteration: {iteration}\")\n",
    "\n",
    "            print(\"------------------------------------------------------\")\n",
    "\n",
    "        ## Record en time\n",
    "        end = time.time()\n",
    "\n",
    "        ## Print execution time\n",
    "        execution_time = end - start\n",
    "        print(f\"Execution time: {execution_time:.2f}\")\n",
    "\n",
    "        ##Print the final solution\n",
    "        print(f\"Best solution: {current_solution}\")\n",
    "        best_solution_fitness = current_fitness\n",
    "        print(f\"Best solution fitness: {best_solution_fitness}\")\n",
    "\n",
    "        ## Prin best fitness over time dict\n",
    "        print(self.best_fitness_overtime)\n",
    "\n",
    "        plt.plot(self.fitness_over_time)\n",
    "        plt.title(\"Best Fitness Over Time\")\n",
    "        plt.xlabel(\"Iteration\")\n",
    "        plt.ylabel(\"Fitness\")\n",
    "        plt.show()\n",
    "\n",
    "def main():\n",
    "    ## Initial Simulated annealing values\n",
    "    init_initial_temp = 100\n",
    "    init_final_temp = 0.01\n",
    "    init_cooling_rate = 0.99\n",
    "    init_max_iterations = 1000\n",
    "    init_kernel_mutation_prob = 0.03\n",
    "\n",
    "    ## Initial hyperparameters\n",
    "    init_solver = ['lbfgs', 'sgd', 'adam']\n",
    "    init_activation = ['identity', 'logistic', 'tanh', 'relu']\n",
    "    init_hidden_layer_sizes = np.arange(20, 200, 1)\n",
    "    init_alpha = np.arange(0.0001, 0.1, 0.001)\n",
    "    init_learning_rate_init = np.arange(0.001, 0.1, 0.001)\n",
    "\n",
    "    ## cooling rate 0.99\n",
    "    sa_optimization = SimulatedAnnealing(initial_temp=init_initial_temp, \n",
    "                                        final_temp=init_final_temp, \n",
    "                                        cooling_rate=init_cooling_rate, \n",
    "                                        max_iterations=init_max_iterations, \n",
    "                                        kernel_mutation_prob=init_kernel_mutation_prob,\n",
    "                                        solver_set=init_solver,\n",
    "                                        activation_set=init_activation,\n",
    "                                        hidden_layer_sizes_set=init_hidden_layer_sizes,\n",
    "                                        alpha_set=init_alpha,\n",
    "                                        learning_rate_init_set=init_learning_rate_init)\n",
    "\n",
    "    sa_optimization.optimization()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "courseworkAdaptiveSystems1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
